{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6433308f",
   "metadata": {},
   "source": [
    "# Data Analysis with Pandas: Core Concepts 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0a1ba",
   "metadata": {},
   "source": [
    "This guide builds on the foundational concepts of Pandas to explore more advanced data manipulation techniques. We will cover how to filter, sort, and group data, as well as how to combine multiple DataFrames through merging and concatenation. Finally, we'll look at how to add and remove columns and rows. All examples use a consistent dataset to demonstrate practical applications of these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb34901-6398-4cd7-87af-012e147a8314",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;66;03m# no input argument needed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir() \u001b[38;5;66;03m# no input argument needed\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmjayant\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNIT\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m31_day_11_08_2025\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd() # no input argument needed\n",
    "os.listdir() # no input argument needed\n",
    "os.chdir('C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\') # full path or the absolute path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "525ac1b5-9e19-4af9-ae33-f953c9c30f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Data_Analysis_with_Pandas_Core_Concepts_04.ipynb',\n",
       " 'Data_Analysis_with_Pandas_Core_Concepts_04.pdf',\n",
       " 'new_transactions.csv',\n",
       " 'sales_transactions_50.csv',\n",
       " 'staff_details.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d97823-1371-48e5-b9de-f308ac44b097",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\Pandas_04_concepts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmjayant\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNIT\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m31_day_11_08_2025\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPandas_04_concepts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\Pandas_04_concepts'"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\Pandas_04_concepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7202f0-15c0-43d5-abf1-0b0c35f623b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Data_Analysis_with_Pandas_Core_Concepts_04.ipynb',\n",
       " 'Data_Analysis_with_Pandas_Core_Concepts_04.pdf',\n",
       " 'new_transactions.csv',\n",
       " 'sales_transactions_50.csv',\n",
       " 'staff_details.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023cf3fa",
   "metadata": {},
   "source": [
    "## Introduction to Pandas Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38508c8d",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Data Manipulation\n",
    "\n",
    "In real-world scenarios, raw data is rarely ready for analysis. Data manipulation involves a series of steps to clean, transform, and structure data to make it suitable for a specific task. Pandas provides a rich set of tools to perform these operations efficiently and intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d10ab9",
   "metadata": {},
   "source": [
    "### 1. DataFrame - Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeabb47",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Filtering Data\n",
    "\n",
    "Filtering is the process of selecting a subset of data from a DataFrame based on specific conditions. This is one of the most common and powerful operations in data analysis, allowing you to focus on the information that is most relevant to your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7822f",
   "metadata": {},
   "source": [
    "#### Example 1: Filtering with a Single Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39146d",
   "metadata": {},
   "source": [
    "Let's find all the sales transactions where the product_type is 'Frozen'. We use a boolean mask to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1595bd56-6963-4126-a932-8be8982f46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sales data\n",
    "# df = pd.read_csv('absolutePath') ## 'C:\\\\Users\\\\mjayant\\\\Documents\\\\NIT\\\\31_day_11_08_2025\\\\'\n",
    "df = pd.read_csv('sales_transactions_50.csv')\n",
    "\n",
    "# Create a boolean mask where the product_type is 'Frozen'\n",
    "is_frozen_mask = df['product_type'] == 'Frozen'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ea10b4-166f-4f90-918c-bc68102ebd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_frozen_mask = df['product_type'] == 'Frozen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7abc9a50-b5ae-4d05-8615-c4b972d5bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_items = df[is_frozen_mask]\n",
    "len(frozen_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e7ee6a-d522-4694-8f22-2291fd24f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>product_type</th>\n",
       "      <th>flavor</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_per_unit_inr</th>\n",
       "      <th>staff_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>3</td>\n",
       "      <td>114.41</td>\n",
       "      <td>EMP002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Kulfi</td>\n",
       "      <td>4</td>\n",
       "      <td>130.38</td>\n",
       "      <td>EMP001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>106</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Tea</td>\n",
       "      <td>5</td>\n",
       "      <td>96.49</td>\n",
       "      <td>EMP004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>3</td>\n",
       "      <td>115.39</td>\n",
       "      <td>EMP004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>111</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>4</td>\n",
       "      <td>104.81</td>\n",
       "      <td>EMP004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>114</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Vanilla</td>\n",
       "      <td>3</td>\n",
       "      <td>187.66</td>\n",
       "      <td>EMP003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
       "0              101       Frozen  Chocolate         3              114.41   \n",
       "4              105       Frozen      Kulfi         4              130.38   \n",
       "5              106       Frozen        Tea         5               96.49   \n",
       "6              107       Frozen     Coffee         3              115.39   \n",
       "10             111       Frozen     Coffee         4              104.81   \n",
       "13             114       Frozen    Vanilla         3              187.66   \n",
       "\n",
       "   staff_id  \n",
       "0    EMP002  \n",
       "4    EMP001  \n",
       "5    EMP004  \n",
       "6    EMP004  \n",
       "10   EMP004  \n",
       "13   EMP003  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_items.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb601d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions for 'Frozen' items:\n",
      "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "0             101       Frozen  Chocolate         3              114.41   \n",
      "4             105       Frozen      Kulfi         4              130.38   \n",
      "5             106       Frozen        Tea         5               96.49   \n",
      "6             107       Frozen     Coffee         3              115.39   \n",
      "8             109      Dessert  Chocolate         4              159.63   \n",
      "\n",
      "  staff_id  \n",
      "0   EMP002  \n",
      "4   EMP001  \n",
      "5   EMP004  \n",
      "6   EMP004  \n",
      "8   EMP002  \n",
      "...\n",
      "Total number of 'Frozen' transactions: 32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sales data\n",
    "df = pd.read_csv('sales_transactions_50.csv')\n",
    "\n",
    "# Create a boolean mask where the product_type is 'Frozen'\n",
    "is_frozen_mask = df['product_type'] == 'Frozen'\n",
    "\n",
    "# Use the mask to filter the DataFrame and display the first few results\n",
    "frozen_items = df[(df['product_type'] == 'Frozen') | (df['quantity'] == 4) ]\n",
    "\n",
    "print(\"Transactions for 'Frozen' items:\")\n",
    "print(frozen_items.head())\n",
    "print(\"...\")\n",
    "print(f\"Total number of 'Frozen' transactions: {len(frozen_items)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38b3cd",
   "metadata": {},
   "source": [
    "This code first creates a Series of True/False values (`is_frozen_mask`) by checking if each row's `product_type` is 'Frozen'. Passing this mask to the DataFrame `df[...]` returns only the rows where the condition is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ccb33",
   "metadata": {},
   "source": [
    "#### Example 2: Filtering with Multiple Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc8e93",
   "metadata": {},
   "source": [
    "Now, let's find all sales of 'Dessert' items with a quantity of more than 3. We combine multiple conditions using logical operators (`&` for AND, `|` for OR, `~` for NOT)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eb49ce9-6d66-4a2d-a272-f9df78e66668",
   "metadata": {},
   "source": [
    "df[<boolean_condition>] \n",
    "boolean_condition --> one condition or multiple conditions \n",
    "one condition --> x > 4 , quantity == 5 , product_type == 'Frozen'\n",
    "multiple condition --> (condition1) <operator> (condition2) , operator can be |(or) & (and) \n",
    "                        (condition1) <operator> (condition2) <operator> (condition3)\n",
    "    if (or) | is used only one of the conditions need to be True\n",
    "    if (and) & all the conditions need to be True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef61eff7-e16e-4b78-85ad-951899ed8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen items or quantity == 5:\n",
      "    transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "1              102     Beverage  Chocolate         1              106.83   \n",
      "5              106       Frozen        Tea         5               96.49   \n",
      "7              108     Beverage      Pista         1              114.69   \n",
      "9              110      Dessert      Pista         5              161.23   \n",
      "11             112     Beverage        Tea         5              189.77   \n",
      "12             113     Beverage        Tea         5              116.68   \n",
      "14             115       Frozen        Tea         5              131.10   \n",
      "16             117       Frozen        Tea         5              124.51   \n",
      "22             123       Frozen    Vanilla         5              149.27   \n",
      "24             125       Frozen      Mango         5              126.97   \n",
      "33             134     Beverage  Chocolate         1              140.82   \n",
      "34             135     Beverage    Vanilla         1              159.58   \n",
      "35             136     Beverage      Kulfi         1              175.84   \n",
      "40             141       Frozen     Coffee         5              124.87   \n",
      "43             144     Beverage    Vanilla         3              129.76   \n",
      "45             146      Dessert      Pista         5              173.77   \n",
      "\n",
      "   staff_id  \n",
      "1    EMP003  \n",
      "5    EMP004  \n",
      "7    EMP001  \n",
      "9    EMP001  \n",
      "11   EMP005  \n",
      "12   EMP005  \n",
      "14   EMP001  \n",
      "16   EMP003  \n",
      "22   EMP003  \n",
      "24   EMP004  \n",
      "33   EMP002  \n",
      "34   EMP003  \n",
      "35   EMP005  \n",
      "40   EMP001  \n",
      "43   EMP003  \n",
      "45   EMP001  \n"
     ]
    }
   ],
   "source": [
    "# Filtering for 'Dessert' items or  a quantity greater than 3\n",
    "beverage_high_qty_items = df[(df['product_type'] == 'Beverage') | (df['quantity'] == 5)]\n",
    "\n",
    "print(\"frozen items or quantity == 5:\")\n",
    "print(beverage_high_qty_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74988b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dessert items with quantity > 3:\n",
      "    transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "8              109      Dessert  Chocolate         4              159.63   \n",
      "9              110      Dessert      Pista         5              161.23   \n",
      "25             126      Dessert      Mango         4              114.22   \n",
      "27             128      Dessert      Pista         4              137.03   \n",
      "28             129      Dessert      Kulfi         4              188.34   \n",
      "45             146      Dessert      Pista         5              173.77   \n",
      "\n",
      "   staff_id  \n",
      "8    EMP002  \n",
      "9    EMP001  \n",
      "25   EMP004  \n",
      "27   EMP001  \n",
      "28   EMP003  \n",
      "45   EMP001  \n"
     ]
    }
   ],
   "source": [
    "# Filtering for 'Dessert' items with a quantity greater than 3\n",
    "dessert_high_qty_items = df[(df['product_type'] == 'Dessert') & (df['quantity'] > 3)]\n",
    "\n",
    "print(\"Dessert items with quantity > 3:\")\n",
    "print(dessert_high_qty_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eca62e",
   "metadata": {},
   "source": [
    "By enclosing each condition in parentheses and joining them with `&`, we create a combined boolean mask. The filtered DataFrame contains only the rows that satisfy both conditions simultaneously. Note that each condition must be enclosed in parentheses to ensure correct operator precedence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff68bd3",
   "metadata": {},
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "A business manager might need to quickly identify all the sales of a particular product line to analyze its performance. For instance, they could filter for all transactions of a 'Frozen' item to see which flavors are most popular or if any are selling poorly. This helps in making informed decisions about inventory and marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5fdf0",
   "metadata": {},
   "source": [
    "### 2. Data Frame - Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98f963",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Sorting Data\n",
    "\n",
    "Sorting a DataFrame by its values is essential for organizing data and making it easier to read and analyze. The `sort_values()` method allows you to sort the DataFrame based on one or more columns, in either ascending or descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25b6b6",
   "metadata": {},
   "source": [
    "#### Example 1: Sorting by a Single Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b6acad",
   "metadata": {},
   "source": [
    "Let's sort our sales transactions by quantity in ascending order to see which items were sold least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8173dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sorted by quantity (ascending, first 5 records):\n",
      "    transaction_id product_type  flavor  quantity  price_per_unit_inr staff_id\n",
      "9              110      Dessert   Pista         5              161.23   EMP001\n",
      "12             113     Beverage     Tea         5              116.68   EMP005\n",
      "14             115       Frozen     Tea         5              131.10   EMP001\n",
      "45             146      Dessert   Pista         5              173.77   EMP001\n",
      "40             141       Frozen  Coffee         5              124.87   EMP001\n"
     ]
    }
   ],
   "source": [
    "# Load the sales data\n",
    "df = pd.read_csv('sales_transactions_50.csv')\n",
    "\n",
    "# Sort the DataFrame by the 'quantity' column in ascending order (default)\n",
    "df_sorted_by_quantity = df.sort_values(by='quantity')\n",
    "\n",
    "print(\"DataFrame sorted by quantity (ascending, first 5 records):\")\n",
    "print(df_sorted_by_quantity.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fa698",
   "metadata": {},
   "source": [
    "The `sort_values()` method, when given a column name, returns a new DataFrame sorted by that column. The default order is ascending, which means the lowest values appear first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d74c1a",
   "metadata": {},
   "source": [
    "#### Example 2: Sorting by Multiple Columns (Descending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb935f",
   "metadata": {},
   "source": [
    "To find the most expensive items that were sold in high quantities, we can sort by `price_per_unit_inr` and then by `quantity`, both in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6ff783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sorted by price (descending) and then quantity (descending, first 5 records):\n",
      "    transaction_id product_type   flavor  quantity  price_per_unit_inr  \\\n",
      "11             112     Beverage      Tea         5              189.77   \n",
      "28             129      Dessert    Kulfi         4              188.34   \n",
      "13             114       Frozen  Vanilla         3              187.66   \n",
      "42             143       Frozen    Mango         3              184.05   \n",
      "35             136     Beverage    Kulfi         1              175.84   \n",
      "\n",
      "   staff_id  \n",
      "11   EMP005  \n",
      "28   EMP003  \n",
      "13   EMP003  \n",
      "42   EMP004  \n",
      "35   EMP005  \n"
     ]
    }
   ],
   "source": [
    "# Sort by 'price_per_unit_inr' and then by 'quantity', both in descending order\n",
    "df_sorted_multi = df.sort_values(by=['price_per_unit_inr', 'quantity'], ascending=[False, False])\n",
    "\n",
    "print(\"DataFrame sorted by price (descending) and then quantity (descending, first 5 records):\")\n",
    "print(df_sorted_multi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff63055",
   "metadata": {},
   "source": [
    "By passing a list of column names to the `by` parameter and a list of boolean values to the `ascending` parameter, we can define a multi-level sort. The data is first sorted by the first column in the list, and then any rows with equal values are sorted by the second column, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb70122-e7dc-4518-a8e9-7d7e91fe66a7",
   "metadata": {},
   "source": [
    "- 1. Prioritizing high-ticket items for sales campaigns.\n",
    "- 2. Identifying high-price, high-stock products for warehouse optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f108606-05a5-423d-adab-23521c0ee3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 products (sorted by price high -> low, then quantity high -> low):\n",
      "   product_id product_name  price_per_unit_inr  quantity\n",
      "0         101      Vanilla               45000        10\n",
      "3         104    Chocolate               15000         5\n",
      "5         106   ChocoChips                2500        30\n",
      "4         105   Strawberry                2500        20\n",
      "2         103  Kesar Pista                1200        30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'product_id': [101, 102, 103, 104, 105, 106],\n",
    "    'product_name': ['Vanilla', 'Mango', 'Kesar Pista', 'Chocolate', 'Strawberry','ChocoChips'],\n",
    "    'price_per_unit_inr': [45000, 800, 1200, 15000, 2500,2500],\n",
    "    'quantity': [10, 50, 30, 5, 20,30]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Multi-column sort: price (descending) → quantity (descending)\n",
    "df_sorted = df.sort_values( by=['price_per_unit_inr', 'quantity'], ascending=[False, False])\n",
    "\n",
    "print(\"Top 5 products (sorted by price high -> low, then quantity high -> low):\")\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599abb94-2ee9-43c4-8791-50b2378191ab",
   "metadata": {},
   "source": [
    "- Sort by Price (High→Low)[desc] → Quantity (Low→High)[asc] </br>\n",
    "**Prioritize expensive items with low stock for inventory clearance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd1ef79-c188-4205-98c4-d153e7b34260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 products (sorted by price high -> low, then quantity low -> high):\n",
      "   product_id product_name  price_per_unit_inr  quantity\n",
      "0         101      Vanilla               45000        10\n",
      "3         104    Chocolate               15000         5\n",
      "4         105   Strawberry                2500        20\n",
      "5         106   ChocoChips                2500        30\n",
      "2         103  Kesar Pista                1200        30\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(\n",
    "    by=['price_per_unit_inr', 'quantity'], \n",
    "    ascending=[False, True]  # Price DESC, Quantity ASC\n",
    ")\n",
    "\n",
    "print(\"Top 5 products (sorted by price high -> low, then quantity low -> high):\")\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cdea9-9856-48a6-a54d-ea9d05d1f12a",
   "metadata": {},
   "source": [
    "- Sort by Quantity (High→Low)[desc] → Price (High→Low)[desc] </br>\n",
    "**Identify high-demand products with premium pricing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96792ad4-a1c6-4425-ae99-1c8b1adc2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 high-demand products with premium pricing (sorted by price high -> low,  quantity high -> low):\n",
      "   product_id product_name  price_per_unit_inr  quantity\n",
      "1         102        Mango                 800        50\n",
      "5         106   ChocoChips                2500        30\n",
      "2         103  Kesar Pista                1200        30\n",
      "4         105   Strawberry                2500        20\n",
      "0         101      Vanilla               45000        10\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(\n",
    "    by=['quantity', 'price_per_unit_inr'], \n",
    "    ascending=[False, False]  # Quantity DESC, Price DESC\n",
    ")\n",
    "print(\"Top 5 high-demand products with premium pricing (sorted by price high -> low,  quantity high -> low):\")\n",
    "\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac866ee3-1208-4022-a3f0-5f4a8c2f1b5c",
   "metadata": {},
   "source": [
    "- Sort by Price (Low→High)[asc] → Quantity (High→Low)[desc] </br>\n",
    "**Find budget-friendly products with high availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7dac2a3-9769-4e01-b0c1-b70256ce08fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 budget-friendly products with high availability (sorted by price low -> high,  quantity high -> low):\n",
      "   product_id product_name  price_per_unit_inr  quantity\n",
      "1         102        Mango                 800        50\n",
      "2         103  Kesar Pista                1200        30\n",
      "5         106   ChocoChips                2500        30\n",
      "4         105   Strawberry                2500        20\n",
      "3         104    Chocolate               15000         5\n"
     ]
    }
   ],
   "source": [
    "df_sorted = df.sort_values(\n",
    "    by=['price_per_unit_inr', 'quantity'], \n",
    "    ascending=[True, False]  # Price ASC, Quantity DESC\n",
    ")\n",
    "\n",
    "print(\"Top 5 budget-friendly products with high availability (sorted by price low -> high,  quantity high -> low):\")\n",
    "\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b88f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "A data analyst might sort the data to generate a sales leaderboard for staff or to identify the most expensive products sold in a day. Sorting helps in quickly identifying patterns, outliers, and trends, which is a key part of any exploratory data analysis process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11c709",
   "metadata": {},
   "source": [
    "### 3. Data Frame - GroupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ff2563",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Grouping Data\n",
    "\n",
    "The `groupby()` method is one of the most powerful features of Pandas. It allows you to split a DataFrame into groups based on some criteria, apply a function (like `sum()`, `mean()`, or `count()`) to each group, and then combine the results. This is similar to the GROUP BY clause in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623e627",
   "metadata": {},
   "source": [
    "#### Example 1: Grouping by a Single Column and Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b43a5d",
   "metadata": {},
   "source": [
    "Let's find the total quantity of each product flavor sold. We'll group the DataFrame by the `flavor` column and then calculate the sum of the `quantity` for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816ee9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total quantity sold per flavor:\n",
      "flavor\n",
      "Brownie        2\n",
      "Chocolate      9\n",
      "Coffee        31\n",
      "Kulfi         21\n",
      "Mango         20\n",
      "Pista         18\n",
      "Strawberry     1\n",
      "Tea           31\n",
      "Vanilla       22\n",
      "Name: quantity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sales data\n",
    "df = pd.read_csv('sales_transactions_50.csv')\n",
    "\n",
    "# Group by 'flavor' and calculate the sum of 'quantity'\n",
    "flavor_sales = df.groupby('flavor')['quantity'].sum()\n",
    "\n",
    "print(\"Total quantity sold per flavor:\")\n",
    "print(flavor_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdfa9a",
   "metadata": {},
   "source": [
    "First, we group the DataFrame by `flavor`. Then, we select the `quantity` column and apply the `sum()` aggregation function. This returns a new Series with the unique flavors as the index and their corresponding total quantities as the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016c8c7",
   "metadata": {},
   "source": [
    "#### Example 2: Grouping by Multiple Columns and Applying Multiple Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613306f",
   "metadata": {},
   "source": [
    "To get a more detailed view, let's group by both `staff_id` and `product_type`, and then find the total `quantity` sold and the total `price_per_unit_inr` for each group. We'll use the `.agg()` method for multiple aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704185eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales summary by staff and product type:\n",
      "                       total_quantity  average_price\n",
      "staff_id product_type                               \n",
      "EMP001   Beverage                   1     114.690000\n",
      "         Dessert                   17     153.584000\n",
      "         Frozen                    25     121.501250\n",
      "EMP002   Beverage                   1     140.820000\n",
      "         Dessert                    6     152.720000\n",
      "         Frozen                     9     129.770000\n",
      "EMP003   Beverage                   5     132.056667\n",
      "         Dessert                   12     154.052500\n",
      "         Frozen                    20     157.964000\n",
      "EMP004   Dessert                    6     113.050000\n",
      "         Frozen                    34     135.663000\n",
      "EMP005   Beverage                  11     160.763333\n",
      "         Dessert                    3     171.640000\n",
      "         Frozen                     5     121.880000\n"
     ]
    }
   ],
   "source": [
    "# Group by 'staff_id' and 'product_type', then aggregate\n",
    "staff_sales_summary = df.groupby(['staff_id', 'product_type']).agg(\n",
    "    total_quantity=('quantity', 'sum'),\n",
    "    average_price=('price_per_unit_inr', 'mean')\n",
    ")\n",
    "\n",
    "print(\"Sales summary by staff and product type:\")\n",
    "print(staff_sales_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d23ef",
   "metadata": {},
   "source": [
    "The `groupby()` method is used with a list of columns. The `.agg()` method takes a dictionary where keys are the new column names and values are tuples of the original column and the aggregation function to apply. This creates a powerful summary table with a multi-index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e6b8f",
   "metadata": {},
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "Grouping data is crucial for generating business intelligence reports. A manager might use this to calculate total revenue per staff member, average sales per hour, or to find the most popular product types on a monthly basis. This aggregated data provides the key insights needed to measure performance and make strategic decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bcedb",
   "metadata": {},
   "source": [
    "### 4. Merging or Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e488ce",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Merging DataFrames\n",
    "\n",
    "Merging or joining is the process of combining two or more DataFrames based on a common column or index. This is a fundamental operation when your data is spread across multiple tables or files, a common scenario in real-world databases. Pandas provides the `merge()` function for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1401647b",
   "metadata": {},
   "source": [
    "#### Example 1: Merging DataFrames with an Inner Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750a8d0",
   "metadata": {},
   "source": [
    "Let's combine our sales_transactions_50 and `staff_details` DataFrames to see which employee made each transaction. We will join them on the common column, `staff_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df22c444-300e-436b-be09-51c24e3f13e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "staff_id\n",
       "EMP001    14\n",
       "EMP004    12\n",
       "EMP003    12\n",
       "EMP002     6\n",
       "EMP005     6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales['staff_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b25405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame (sales with staff names, first 5 records):\n",
      "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "0             101       Frozen  Chocolate         3              114.41   \n",
      "1             102     Beverage  Chocolate         1              106.83   \n",
      "2             103      Dessert    Brownie         2              111.88   \n",
      "3             104      Dessert     Coffee         2              145.81   \n",
      "4             105       Frozen      Kulfi         4              130.38   \n",
      "\n",
      "  staff_id staff_name joining_date shift_type  \n",
      "0   EMP002      Priya   2023-02-20    Evening  \n",
      "1   EMP003      Rohan   2023-03-10    Morning  \n",
      "2   EMP004      Sneha   2023-04-01    Evening  \n",
      "3   EMP002      Priya   2023-02-20    Evening  \n",
      "4   EMP001      Aarav   2023-01-15    Morning  \n",
      "50\n",
      "5\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two DataFrames\n",
    "df_sales = pd.read_csv('sales_transactions_50.csv')\n",
    "df_staff = pd.read_csv('staff_details.csv')\n",
    "\n",
    "# Perform an inner merge on the 'staff_id' column\n",
    "merged_df = pd.merge(df_sales, df_staff, on='staff_id', how='inner')\n",
    "\n",
    "print(\"Merged DataFrame (sales with staff names, first 5 records):\")\n",
    "print(merged_df.head())\n",
    "\n",
    "print(len(df_sales))\n",
    "print(len(df_staff))\n",
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3138728",
   "metadata": {},
   "source": [
    "The `pd.merge()` function is used here. We specify the two DataFrames to merge, the column to join on (`on='staff_id'`), and the type of join (`how='inner'`). An inner join only keeps rows where the `staff_id` exists in both DataFrames, which is the default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad8f9f",
   "metadata": {},
   "source": [
    "#### Example 2: Merging with a Left Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d47cc6",
   "metadata": {},
   "source": [
    "What if we wanted to keep all sales transactions, even if a `staff_id` in the sales data didn't have a match in the staff details? A left join would be appropriate. In this case, since all `staff_id`s in our sales data have a match, the output will be the same as the inner join, but the principle is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2fed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left merged DataFrame:\n",
      "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "0             101       Frozen  Chocolate         3              114.41   \n",
      "1             102     Beverage  Chocolate         1              106.83   \n",
      "2             103      Dessert    Brownie         2              111.88   \n",
      "3             104      Dessert     Coffee         2              145.81   \n",
      "4             105       Frozen      Kulfi         4              130.38   \n",
      "\n",
      "  staff_id staff_name joining_date shift_type  \n",
      "0   EMP002      Priya   2023-02-20    Evening  \n",
      "1   EMP003      Rohan   2023-03-10    Morning  \n",
      "2   EMP004      Sneha   2023-04-01    Evening  \n",
      "3   EMP002      Priya   2023-02-20    Evening  \n",
      "4   EMP001      Aarav   2023-01-15    Morning  \n"
     ]
    }
   ],
   "source": [
    "# Perform a left merge on the 'staff_id' column\n",
    "left_merged_df = pd.merge(df_sales, df_staff, on='staff_id', how='left')\n",
    "\n",
    "print(\"Left merged DataFrame:\")\n",
    "print(left_merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e49f3",
   "metadata": {},
   "source": [
    "A left merge (`how='left'`) returns all rows from the \"left\" DataFrame (`df_sales`) and the matching rows from the \"right\" DataFrame (`df_staff`). If there's no match, the columns from the right DataFrame are filled with `NaN` for that row. This is useful for preserving all records from a primary dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2a0d33",
   "metadata": {},
   "source": [
    "#### Pros and Cons of Merging\n",
    "\n",
    "* Pros:\n",
    "\n",
    "Versatility: Allows for powerful combinations of data using different join types (inner, left, right, outer).\n",
    "Clarity: The `on` parameter makes the join condition explicit and easy to understand.\n",
    "* Cons:\n",
    "\n",
    "Key Dependency: Requires a common key or column between the DataFrames to be effective.\n",
    "Memory Usage: Creating a new, combined DataFrame can be memory-intensive with very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49337c",
   "metadata": {},
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "Merging is essential for getting a complete picture of a business's operations. A sales manager might merge sales transactions with staff details to analyze sales performance per employee. They could also merge sales data with a separate product catalog to see how many units of a specific item were sold, regardless of the flavor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e485edb",
   "metadata": {},
   "source": [
    "### 5. DataFrame - Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71645bf1",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Concatenating DataFrames\n",
    "\n",
    "Concatenation is the process of stacking DataFrames on top of each other (vertically) or side by side (horizontally). It is used when you have multiple DataFrames with the same or similar structure that you want to combine into a single, larger DataFrame. The `pd.concat()` function is used for this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dc827",
   "metadata": {},
   "source": [
    "#### Example 1: Concatenating Vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf43cd",
   "metadata": {},
   "source": [
    "Let's add the new sales transactions from `new_transactions.csv` to our main `sales_transactions_50.csv` DataFrame. We will stack the new data below the old data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310b74d-8bd7-42e0-8362-60ed128b2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 -->  col1,col2,col3 (50)\n",
    "df2 -->  col1,col2,col3 (10)\n",
    "----------------------------\n",
    "vertical stacking - one over the other \n",
    "df3 --> pd.concat(df1,df2)\n",
    "df3  ---> col1 , col2, col3 (60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c660e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sales DataFrame tail:\n",
      "    transaction_id product_type   flavor  quantity  price_per_unit_inr  \\\n",
      "47             148      Dessert  Vanilla         3              163.52   \n",
      "48             149      Dessert    Pista         2              110.91   \n",
      "49             150       Frozen    Kulfi         4              144.14   \n",
      "\n",
      "   staff_id  \n",
      "47   EMP003  \n",
      "48   EMP003  \n",
      "49   EMP005  \n",
      "\n",
      "New transactions DataFrame:\n",
      "   transaction_id product_type      flavor  quantity  price_per_unit_inr  \\\n",
      "0             151       Frozen       Kulfi         2               180.0   \n",
      "1             152      Dessert        Cake         1               100.0   \n",
      "2             153       Frozen  Strawberry         3               140.0   \n",
      "3             154     Beverage    Iced Tea         2               110.0   \n",
      "4             155      Dessert     Brownie         1               120.0   \n",
      "\n",
      "  staff_id  \n",
      "0   EMP001  \n",
      "1   EMP002  \n",
      "2   EMP004  \n",
      "3   EMP003  \n",
      "4   EMP005  \n",
      "\n",
      "Combined DataFrame after concatenation (tail):\n",
      "    transaction_id product_type      flavor  quantity  price_per_unit_inr  \\\n",
      "45             146      Dessert       Pista         5              173.77   \n",
      "46             147       Frozen       Mango         1              157.57   \n",
      "47             148      Dessert     Vanilla         3              163.52   \n",
      "48             149      Dessert       Pista         2              110.91   \n",
      "49             150       Frozen       Kulfi         4              144.14   \n",
      "50             151       Frozen       Kulfi         2              180.00   \n",
      "51             152      Dessert        Cake         1              100.00   \n",
      "52             153       Frozen  Strawberry         3              140.00   \n",
      "53             154     Beverage    Iced Tea         2              110.00   \n",
      "54             155      Dessert     Brownie         1              120.00   \n",
      "\n",
      "   staff_id  \n",
      "45   EMP001  \n",
      "46   EMP004  \n",
      "47   EMP003  \n",
      "48   EMP003  \n",
      "49   EMP005  \n",
      "50   EMP001  \n",
      "51   EMP002  \n",
      "52   EMP004  \n",
      "53   EMP003  \n",
      "54   EMP005  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two DataFrames\n",
    "df_sales = pd.read_csv('sales_transactions_50.csv')\n",
    "df_new_sales = pd.read_csv('new_transactions.csv')\n",
    "\n",
    "# Vertically concatenate the two DataFrames\n",
    "combined_sales = pd.concat([df_sales, df_new_sales], ignore_index=True)\n",
    "\n",
    "print(\"Original sales DataFrame tail:\")\n",
    "print(df_sales.tail(3))\n",
    "print(\"\\nNew transactions DataFrame:\")\n",
    "print(df_new_sales)\n",
    "print(\"\\nCombined DataFrame after concatenation (tail):\")\n",
    "print(combined_sales.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2ddb2",
   "metadata": {},
   "source": [
    "The `pd.concat()` function takes a list of DataFrames to combine. By default, it concatenates vertically along the row axis (`axis=0`). We use `ignore_index=True` to reset the index of the final DataFrame, creating a clean, continuous index from 0 upwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fa9df",
   "metadata": {},
   "source": [
    "#### Pros and Cons of Concatenation\n",
    "\n",
    "* Pros:\n",
    "\n",
    "Simplicity: It's a straightforward way to stack or join DataFrames with similar structures.\n",
    "Performance: Generally faster than other join methods when simply stacking data.\n",
    "* Cons:\n",
    "\n",
    "Schema Dependency: Works best when the DataFrames have the same columns and data types.\n",
    "Index Duplication: Without `ignore_index=True`, the final DataFrame can have duplicate index values, which can cause issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a535e95",
   "metadata": {},
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "Concatenation is often used to combine daily, weekly, or monthly sales reports into a single, comprehensive dataset for long-term analysis. For example, a business might receive a new CSV file of transactions every day, and a script could use `pd.concat()` to append this new data to a master sales file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b3b82",
   "metadata": {},
   "source": [
    "### 6. DataFrame - Adding, Dropping Columns & Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442996c",
   "metadata": {},
   "source": [
    "#### Concept Introduction: Modifying DataFrame Structure\n",
    "\n",
    "As part of data cleaning and preparation, you will often need to add, remove, or modify columns and rows in a DataFrame. These operations are essential for feature engineering, data normalization, and preparing a dataset for a specific analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb2b5b3",
   "metadata": {},
   "source": [
    "#### Example 1: Adding a New Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e33eb5",
   "metadata": {},
   "source": [
    "Let's add a new column, `total_price_inr`, to our sales DataFrame. This new column will be the result of a simple calculation: `quantity` multiplied by `price_per_unit_inr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bfc5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with the new 'total_price_inr' column (first 5 records):\n",
      "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "0             101       Frozen  Chocolate         3              114.41   \n",
      "1             102     Beverage  Chocolate         1              106.83   \n",
      "2             103      Dessert    Brownie         2              111.88   \n",
      "3             104      Dessert     Coffee         2              145.81   \n",
      "4             105       Frozen      Kulfi         4              130.38   \n",
      "\n",
      "  staff_id  total_price_inr  \n",
      "0   EMP002           343.23  \n",
      "1   EMP003           106.83  \n",
      "2   EMP004           223.76  \n",
      "3   EMP002           291.62  \n",
      "4   EMP001           521.52  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sales_transactions_50.csv')\n",
    "\n",
    "# Add a new column by multiplying existing columns\n",
    "df['total_price_inr'] = df['quantity'] * df['price_per_unit_inr']\n",
    "\n",
    "print(\"DataFrame with the new 'total_price_inr' column (first 5 records):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccb048",
   "metadata": {},
   "source": [
    "Adding a new column is as simple as assigning a new `Series` of values to a new column name. This new column is created and populated based on the calculation of the two existing columns. This is a very common task for feature creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9691c",
   "metadata": {},
   "source": [
    "#### Example 2: Dropping Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74be761",
   "metadata": {},
   "source": [
    "Sometimes, columns are no longer needed. We can use the `drop()` method to remove one or more columns from a DataFrame. Let's drop the `product_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6598e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping the 'product_type' column (first 5 records):\n",
      "   transaction_id     flavor  quantity  price_per_unit_inr staff_id  \\\n",
      "0             101  Chocolate         3              114.41   EMP002   \n",
      "1             102  Chocolate         1              106.83   EMP003   \n",
      "2             103    Brownie         2              111.88   EMP004   \n",
      "3             104     Coffee         2              145.81   EMP002   \n",
      "4             105      Kulfi         4              130.38   EMP001   \n",
      "\n",
      "   total_price_inr  \n",
      "0           343.23  \n",
      "1           106.83  \n",
      "2           223.76  \n",
      "3           291.62  \n",
      "4           521.52  \n"
     ]
    }
   ],
   "source": [
    "# Drop the 'product_type' column\n",
    "df_dropped_col = df.drop(columns='product_type')\n",
    "\n",
    "print(\"DataFrame after dropping the 'product_type' column (first 5 records):\")\n",
    "print(df_dropped_col.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c55ab9",
   "metadata": {},
   "source": [
    "The `drop()` method returns a new DataFrame with the specified column(s) removed. You must specify `axis=1` (or `columns=`) to tell Pandas you are dropping a column and not a row. By default, `drop()` returns a new DataFrame, leaving the original unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144de469",
   "metadata": {},
   "source": [
    "#### Example 3: Dropping Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c50e96",
   "metadata": {},
   "source": [
    "We can also drop rows based on their index. Let's drop the first and the last rows of our DataFrame, which correspond to index labels 0 and 49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd211e6-e5f4-49f8-815a-ec9981b15425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>product_type</th>\n",
       "      <th>flavor</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_per_unit_inr</th>\n",
       "      <th>staff_id</th>\n",
       "      <th>total_price_inr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>3</td>\n",
       "      <td>114.41</td>\n",
       "      <td>EMP002</td>\n",
       "      <td>343.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Beverage</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>1</td>\n",
       "      <td>106.83</td>\n",
       "      <td>EMP003</td>\n",
       "      <td>106.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>Brownie</td>\n",
       "      <td>2</td>\n",
       "      <td>111.88</td>\n",
       "      <td>EMP004</td>\n",
       "      <td>223.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Dessert</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>145.81</td>\n",
       "      <td>EMP002</td>\n",
       "      <td>291.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Frozen</td>\n",
       "      <td>Kulfi</td>\n",
       "      <td>4</td>\n",
       "      <td>130.38</td>\n",
       "      <td>EMP001</td>\n",
       "      <td>521.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
       "0             101       Frozen  Chocolate         3              114.41   \n",
       "1             102     Beverage  Chocolate         1              106.83   \n",
       "2             103      Dessert    Brownie         2              111.88   \n",
       "3             104      Dessert     Coffee         2              145.81   \n",
       "4             105       Frozen      Kulfi         4              130.38   \n",
       "\n",
       "  staff_id  total_price_inr  \n",
       "0   EMP002           343.23  \n",
       "1   EMP003           106.83  \n",
       "2   EMP004           223.76  \n",
       "3   EMP002           291.62  \n",
       "4   EMP001           521.52  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a673be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping the first and last rows (head and tail):\n",
      "   transaction_id product_type     flavor  quantity  price_per_unit_inr  \\\n",
      "1             102     Beverage  Chocolate         1              106.83   \n",
      "2             103      Dessert    Brownie         2              111.88   \n",
      "\n",
      "  staff_id  total_price_inr  \n",
      "1   EMP003           106.83  \n",
      "2   EMP004           223.76  \n",
      "...\n",
      "    transaction_id product_type   flavor  quantity  price_per_unit_inr  \\\n",
      "47             148      Dessert  Vanilla         3              163.52   \n",
      "48             149      Dessert    Pista         2              110.91   \n",
      "\n",
      "   staff_id  total_price_inr  \n",
      "47   EMP003           490.56  \n",
      "48   EMP003           221.82  \n"
     ]
    }
   ],
   "source": [
    "# Drop rows at index 0 and 49\n",
    "df_dropped_rows = df.drop(index=[0, 49])\n",
    "\n",
    "print(\"DataFrame after dropping the first and last rows (head and tail):\")\n",
    "print(df_dropped_rows.head(2))\n",
    "print(\"...\")\n",
    "print(df_dropped_rows.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340646cd",
   "metadata": {},
   "source": [
    "To drop rows, we use the `index` parameter and pass a list of index labels. This method is useful when you want to remove specific records from the dataset, for example, removing a transaction that was found to be fraudulent or duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade608e3",
   "metadata": {},
   "source": [
    "#### Real-time Usage\n",
    "\n",
    "Data modification is a daily task for an analyst. They might add a new column for 'Profit_Margin' to a sales DataFrame to perform a new analysis. They might also drop a column like 'transaction_id' before training a machine learning model, as it is unlikely to have predictive value. Dropping rows is useful for removing outliers or duplicate entries from a dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
